# DART: A Hierarchical Transformer for Document-level Aspect-based Sentiment Classification

<div align="center">

[![Python Versions](https://img.shields.io/badge/-Python_3.7_%7C_3.8_%7C_3.9_%7C_3.10-blue?logo=python&logoColor=white)](https://www.python.org/downloads/)
[![pytorch](https://img.shields.io/badge/PyTorch_1.12+-ee4c2c?logo=pytorch&logoColor=white)](https://pytorch.org/get-started/locally/)
[![Lightning](https://img.shields.io/badge/-Lightning_1.7+-792ee5?logo=pytorchlightning&logoColor=white)](https://pytorchlightning.ai/)
[![license](https://img.shields.io/badge/License-Apache%202.0-blue.svg?labelColor=gray)](https://github.com/YanZehong/dart#license)
</div>

This work introduces a hierarchical Transformer-based architecture called **DART** (**D**ocument-level **A**spect-based **R**epresentation from **T**ransformers) which effectively encodes information at three levels, namely token, sentence, and document level. DART employs an attentive aggregation mechanism to learn aspect-specific document representation for sentiment classification.

## Table of Contents
- [Project Structure](#project-structure)  
- [Quickstart](#quickstart-)  
  * [Installation](#installation)  
  * [Fine-tuning with DART](#fine-tuning-with-dart)  
  * [Evaluate checkpoint on test dataset](#evaluate-checkpoint-on-test-dataset)  
- [Introduction](#introduction)  
  * [DART Architecture](#dart-architecture)  
  * [Datasets](#datasets)  
  * [Comparative Study](#comparative-study)
- [FAQ](#faq)  
- [Citation](#citation)  

## Project Structure
The directory structure of this project is:
```
â”œâ”€â”€ configs            <- Hydra configuration files
â”‚   â”œâ”€â”€ logdir            <- Logger configs
â”‚   â”œâ”€â”€ data              <- Datamodule configs
â”‚   â”œâ”€â”€ model             <- Modelmodule configs
â”‚   â”œâ”€â”€ experiment        <- Experiment configs
â”‚   â””â”€â”€ cfg.yaml          <- Main config for training
â”‚
â”œâ”€â”€ dataset            <- Project data
â”œâ”€â”€ datamodules        <- Datamodules (TripAdvisor, BeerAdvocate, PerSenT)
â”œâ”€â”€ models             <- Models (DART, Longformer, BigBird)
â”œâ”€â”€ logs               <- Logs generated by hydra and lightning loggers
â”œâ”€â”€ outputs            <- Save generated data
â”œâ”€â”€ utils              <- Utility scripts
â”‚
â”œâ”€â”€ run.py             <- Run Training and evaluation
â””â”€â”€ README.md
``` 

## Quickstart ðŸš€

### Installation

Step 0. Download and install Miniconda from the [official website](https://docs.conda.io/en/latest/miniconda.html).  
Step 1. Install DART and dependencies.  

```
# clone project
git clone https://github.com/YanZehong/dart
cd dart

# [OPTIONAL] create conda environment and activate it
conda create -n myenv -y python=3.10 pip
conda activate myenv

# install pytorch according to instructions
# https://pytorch.org/get-started/
# conda install pytorch torchvision torchaudio pytorch-cuda=11.6 -c pytorch -c nvidia

# install requirements
pip install -r requirements.txt
```

> **Note**: To install requirements, run `pip install -r requirements.txt`. Please ensure that you have met the prerequisites in [PyTorch](https://pytorch.org/) and install correspond version. 


### Fine-tuning with DART
Train model with chosen experiment configuration from [configs/experiment/](configs/experiment/). For different datasets, please use the recommended experimental settings (`tripadvisor-dart`, `beeradvocate-dart` and `persent-dart`).

```bash
python run.py experiment=tripadvisor-dart gpu=1
```

Train model with default configuration
```bash
# train on 1 GPU
python run.py gpu=1

# train with DDP (Distributed Data Parallel) (3 GPUs)
python run.py gpu=[0,1,2]
```
> **Warning**: Currently there are problems with DDP mode, read [this issue](https://github.com/Lightning-AI/lightning/issues/8375) to learn more.

You can override any parameter from command line like this
```bash
python run.py gpu=3 train.num_epochs=10 train.batch_size=32
```

> **Note** When you only specify some variables, other values/parameters will use default setting from [configs/cfg.yaml](configs/cfg.yaml).

### Evaluate checkpoint on test dataset  

You can find fine-tuned checkpoints in our [Google Drive](https://drive.google.com/drive/folders/1OAJw4dLMSe5ySM2QUy2lBtNPgFd74k1c?usp=share_link).  
We recommend using the following checkpoints :  
|  Dataset   |                                   Fine-tuned checkpoint                                   |   Size   | Accuracy | 
| :------- | :----------------------------------------------------------------------------------------- | :-------: |:----------------: |
|trip_advisor | epoch=3-step=8694.ckpt | 1962MB | 86.36% |
|beer_advocate | epoch=3-step=5936.ckpt | 1962MB | 88.13% |
|persent | epoch=0-step=88.ckpt | 1962MB | 83.40% |

```bash
# Optionally, you can download them using wget or gdown as
pip install gdown
gdown --folder https://drive.google.com/drive/folders/1OAJw4dLMSe5ySM2QUy2lBtNPgFd74k1c?usp=share_link
# Next, unzip it to the directory `ckpt_path`
# And then specify the corresponding dataset 
# by setting DATA_NAME as trip_advisor/beer_advocate/persent

# evaluate on 1 GPU
python eval.py gpu=1 data=DATA_NAME ckpt_path='/path/to/ckpt/name.ckpt'

# evaluate on cpu
python eval.py ckpt_path='/path/to/ckpt/name.ckpt'
```

> **Note**: If you get an error `mismatched input '=' expecting <EOF>`, use the escape character '\=' to fix this problem. Or you can specify the value of `ckpt_path` in [configs/cfg.yaml](configs/cfg.yaml). Consider visiting that [gdown page](https://github.com/wkentaro/gdown) for full instructions, since the source repo may have more up-to-date instructions.

<details>
<summary><b>Use Miniconda for GPU environments</b></summary>

Use miniconda for your python environments (it's usually unnecessary to install full anaconda environment, miniconda should be enough).
It makes it easier to install some dependencies, like cudatoolkit for GPU support. It also allows you to access your environments globally.

Example installation:

```bash
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash Miniconda3-latest-Linux-x86_64.sh
```

Create new conda environment:

```bash
conda create -n myenv python=3.10
conda activate myenv
```

</details>

<details>
<summary><b>Use torchmetrics</b></summary>

Use official [torchmetrics](https://github.com/PytorchLightning/metrics) library to ensure proper calculation of metrics. This is especially important for multi-GPU training!

For example, instead of calculating accuracy by yourself, you should use the provided `Accuracy` class like this:

```python
from torchmetrics import Accuracy

class ModelModel(LightningModule):
    def __init__(self)
        self.train_acc = Accuracy()
        self.val_acc = Accuracy()

    def training_step(self, batch, batch_idx):
        ...
        acc = self.train_acc(predictions, targets)
        self.log("train/acc", acc)
        ...

    def validation_step(self, batch, batch_idx):
        ...
        acc = self.val_acc(predictions, targets)
        self.log("val/acc", acc)
        ...
```

Make sure to use different metric instance for each step to ensure proper value reduction over all GPU processes.

Torchmetrics provides metrics for most use cases, like F1 score or confusion matrix. Read [documentation](https://torchmetrics.readthedocs.io/en/latest/#more-reading) for more.

</details>

<details>
<summary><b>Follow PyTorch Lightning style guide</b></summary>

The style guide is available [here](https://pytorch-lightning.readthedocs.io/en/latest/starter/style_guide.html).<br>
</details>

## Introduction

### DART Architecture

<p align="center">
<img width="645" alt="Fig-3_DART" src="https://user-images.githubusercontent.com/39628004/209788430-f124b814-f70a-41a8-9335-3c5d0ff371a0.png">
</p>

Overview of the model: DART overcomes the restriction of 512 tokens by splitting the long document into sentences or chunks of less than 512 tokens, and processing each sentence/chunk before aggregating the results. The proposed DART framework takes as input a document $d$ and an aspect $a_j$ and output the document representation $\hat{d}_j$ with respect to $a_j$.
There are three key blocks in DART:

- Sentence Encoding Block.  
- Global Context Interaction Block.  
- Aspect Aggregation Block.  

### Datasets

Experiments on multiple datasets including a curated dataset of long documents on social issues. Additionally, you can run fine-tuning of the downloaded model on your dataset of interest.

|      Dataset      |      \#aspects      |      \#docs      | \#long docs (\%)| \#sentences/doc |  \#tokens/doc | \#tokens/sentence
|:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|
| TripAdvisor | 7 | 28543 | 4027 (14.1\%) | 12.9 | 298.9 | 23.1|
| BeerAdvocate| 4 | 27583 | 217 (0.8\%) | 11.1 | 173.5 | 15.7|
| PerSenT | 6 | 4512 | 1031 (22.9\%) | 17.5 | 389.8 | 22.2  |

### Comparative Study
  - Non-transformer-based Methods:
    - LRR [[paper](https://dl.acm.org/doi/abs/10.1145/1835804.1835903)] [[docs](https://github.com/biubiutang/LARA-1)]  
    - N-DMSC [[paper](https://aclanthology.org/D17-1217/)] [[docs](https://github.com/HKUST-KnowComp/DMSC)]  
    - VWS-DMSC [[paper](https://aclanthology.org/N19-1036/)] [[docs](https://github.com/HKUST-KnowComp/VWS-DMSC)]  
    - D-MILN [[paper](https://aclanthology.org/2020.emnlp-main.570/)]   
  - Trainsformer-based Methods:
    - Longformer [[paper](https://arxiv.org/abs/2004.05150)] [[docs](https://github.com/allenai/longformer)]  
    - Big Bird [[paper](https://arxiv.org/abs/2007.14062)] [[docs](https://github.com/google-research/bigbird)]  

## FAQ

#### What license is this library released under?

All code *and* models are released under the Apache 2.0 license. See the
`LICENSE` file for more information.

#### I am getting out-of-memory errors, what is wrong?
All experiments in the paper were fine-tuned on a GPU/GPUs, which has 40GB of device RAM. Therefore, when using a GPU with 12GB - 16GB of RAM, you are likely to encounter out-of-memory issues if you use the same hyperparameters described in the paper. Additionally, different models require different amount of memory. Available memory also depends on the accelerator configuration (both type and count).

The factors that affect memory usage are:  
-   **`data.max_num_seq`**: You can fine-tune with a shorter max sequence length to save
    substantial memory. 

-   **`train.batch_size`**: The memory usage is also directly proportional to
    the batch size. You could decrease the `train.batch_size=8` (and decrease `train.lr`
    accordingly) if you encounter an out-of-memory error.

-   **`model.backbone`, `base` vs. `large`**: The `large` model
    requires significantly more memory than `base`.


## Citation

If you find this work useful, please cite as following:

```
@article{2023dart,
  title={DART: A Hierarchical Transformer for Document-level Aspect-based Sentiment Classification},
  author={},
  journal={},
  year={}
}
```

If we submit the paper to a conference or journal, we will update the BibTeX.


## Contact information

For help or issues using DART, please submit a GitHub issue.

For personal communication related to DART, please contact [me](mailto:yanzehong1101@outlook.com).
<table>
  <tr>
    <td align="center"><a href="https://github.com/YanZehong"><img src="https://github.com/YanZehong.png" width="100px;" alt=""/><br /><sub><b>Yan Zehong</b></sub></a><br /><a href="https://github.com/YanZehong/dart" title="Code">ðŸ’»</a></td>
  </tr>
</table>
